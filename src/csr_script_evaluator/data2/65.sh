#!/bin/bash
# Training
python finetune.py --base_model='decapoda-research/llama-7b-hf' --data_path='yahma/alpaca-cleaned' --output_dir='./lora-alpaca' --batch_size=128 --micro_batch_size=4 --num_epochs=1 --learning_rate=1e-4 --cutoff_len=512 --val_set_size=2000 --lora_r=8 --lora_alpha=16 --lora_dropout=0.05 --lora_target_modules='[q_proj,v_proj]' --train_on_inputs --group_by_length

# Inference / Demonstration
python generate.py --load_8bit --base_model='decapoda-research/llama-7b-hf' --lora_weights='tloen/alpaca-lora-7b'

# Testing / Evaluation

```